# MACHINE LEARNING GROUP PROJECT
# JOY BARNO
# DULANI PALIHAPITIYA
# ROBERT SHERRICK



import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random as rd
from PIL import Image




def init_params(input_size, hidden_units):
    W1 = np.random.randn(hidden_units, input_size) * 0.01
    b1 = np.zeros((hidden_units, 1))
    W2 = np.random.randn(2, hidden_units) * 0.01  # 2 output neurons (binary classification)
    b2 = np.zeros((2, 1))
    return W1, b1, W2, b2

def ReLU(Z):
    return np.maximum(0, Z)

def softmax(Z):
    exp = np.exp(Z - np.max(Z))
    return exp / exp.sum(axis=0, keepdims=True)

def forward_prop(W1, b1, W2, b2, X):
    Z1 = W1.dot(X) + b1
    A1 = ReLU(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def one_hot(Y):
    one_hot_Y = np.zeros((2, Y.size))
    one_hot_Y[Y, np.arange(Y.size)] = 1
    return one_hot_Y

def back_prop(Z1, A1, Z2, A2, W2, X, Y):
    m = Y.size
    one_hot_Y = one_hot(Y)
    dZ2 = A2 - one_hot_Y
    dW2 = (1/m) * dZ2.dot(A1.T)
    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = W2.T.dot(dZ2) * (Z1 > 0)
    dW1 = (1/m) * dZ1.dot(X.T)
    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)
    return dW1, db1, dW2, db2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
    W1 -= alpha * dW1
    b1 -= alpha * db1
    W2 -= alpha * dW2 
    b2 -= alpha * db2
    return W1, b1, W2, b2

def get_accuracy(predictions, Y):
    return np.mean(predictions == Y)

def evaluate_model(X_test, t_test, W1, b1, W2, b2):
    # Forward propagation on test set
    _, _, _, A2_test = forward_prop(W1, b1, W2, b2, X_test)
    test_preds = np.argmax(A2_test, axis=0)
    
    # Calculate metrics
    accuracy = np.mean(test_preds == t_test)
    print(f"Final Test Accuracy: {accuracy:.2%}")
    
    # Confusion matrix
    #cm = confusion_matrix(t_test, test_preds)
   # print("Confusion Matrix:")
    #print(cm)
    
    return accuracy


# Just for reproducing results
#rd.seed(0)

# Load images from specific directory, compresses them to a smaller size, then converts them to usable values
def load_compress_worms(directory, label, target_size=(101,101)):
	images = []
	labels = []
	filenames = []

	# Search through every file in specified directory
	for filename in os.listdir(directory):
		# Check if file is a png image
		if filename.endswith('.png'):
			img_path = os.path.join(directory, filename)	# Get full path to image
			img = Image.open(img_path)						# Open image

			if img.size != target_size:						# Resize if not already in target size
				img = img.resize(target_size)

			img_array = np.array(img) / 255.0				# Put image pixel values into range of [0, 1)

			flattened_img = img_array.flatten()				# Flatten image into 1-D array

			images.append(flattened_img)
			labels.append(label)
			filenames.append(filename)

	return np.array(images), np.array(labels), filenames

def visualize_image(X, t, target_size):
	idx = rd.randint(0, 5499)
	plt.figure(figsize=(10,3))
	img = X[idx].reshape(target_size)
	plt.imshow(img, cmap='gray')
	plt.title(f't={t} image number {idx}')
	plt.axis('off')
	plt.show()


def visualize_split(t_train, t_test):
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.pie(np.unique(t_train, return_counts=True)[1], 
            labels=['No Worms', 'Worms'], autopct='%1.1f%%')
    plt.title('Training Set Distribution')
    
    plt.subplot(1, 2, 2)
    plt.pie(np.unique(t_test, return_counts=True)[1], 
            labels=['No Worms', 'Worms'], autopct='%1.1f%%')
    plt.title('Test Set Distribution')
    
    plt.tight_layout()
    plt.show()


# After combining datasets (X and t)
# X shape= (4096, m), t shape: (m,)

def train_test_split(X, t, test_size=0.2):
    # Set random seed for reproducibility
    #np.random.seed(random_state)
    
    # Create shuffled indices
    m = X.shape[1]
    indices = np.random.permutation(m)
    
    # Calculate split index
    split = int(m * (1 - test_size))
    
    # Split indices
    train_idx = indices[:split]
    test_idx = indices[split:]
    
    # Split data
    X_train = X[:, train_idx]
    t_train = t[train_idx]
    X_test = X[:, test_idx]
    t_test = t[test_idx]
    
    return X_train, X_test, t_train, t_test


def main():
	parser = argparse.ArgumentParser()
	parser.add_argument('-n', '--none')
	parser.add_argument('-w', '--worms')
	args = parser.parse_args()

	size = (80,80)
	no_worms_dir = 'C:/DESKTOP_SHIT/Machine Learning/Celegans_ModelGen/0'
	worms_dir = 'C:/DESKTOP_SHIT/Machine Learning/Celegans_ModelGen/1'

	# THESE DIRECTORIES WORK ON MY PC - MUST BE CHANGED TO ASK USER
	# no_worms_dir = '/home/rbs/Insync/School/Spring 2025/ECE 5370/Project 4/Celegans_ModelGen/0/'
	# worms_dir = '/home/rbs/Insync/School/Spring 2025/ECE 5370/Project 4/Celegans_ModelGen/1/'

	print("Reading no worms...")
	X_no_worms, t_no_worms, no_worms_names = load_compress_worms(no_worms_dir, 0, target_size=size)
	print(f"Images read: {X_no_worms.shape[0]}")
	print(f"Image sizes: {X_no_worms.shape[1]}")

	print("Reading worms...")
	X_worms, t_worms, worms_names = load_compress_worms(worms_dir, 1, target_size=size)
	print(f"Images read: {X_worms.shape[0]}")
	print(f"Image sizes: {X_worms.shape[1]}")

	visualize_image(X_no_worms, t_no_worms, size)
	visualize_image(X_worms, t_worms, size)
	
	X = np.hstack((X_no_worms.T, X_worms.T))  # (4096, m)
	t = np.concatenate((t_no_worms, t_worms))
    
    # Split into train/test
	X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2)
    
    # Split train into train/val
	X_train, X_val, t_train, t_val = train_test_split(X_train, t_train, test_size=0.25)
    # Combine datasets
	
    # Transpose to match neural network input format (features, samples)
	

    # Split dataset
	m = t.size
	indices = np.random.permutation(m)
	dev_size = int(0.2 * m)  # 20% validation set

	X_dev = X[:, indices[:dev_size]]
	t_dev = t[indices[:dev_size]]
	X_train = X[:, indices[dev_size:]]
	t_train = t[indices[dev_size:]]

	# Initialize network parameters
	input_size = X.shape[0]  # 64x64 = 4096
	hidden_units = 128
	W1, b1, W2, b2 = init_params(input_size, hidden_units)

    # Training parameters
	alpha = 0.001
	iterations = 500

    # Training loop
	for i in range(iterations):
		Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_train)
		dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X_train, t_train)
		W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)
        
		if i % 100 == 0:
			predictions = np.argmax(A2, axis=0)
			acc = get_accuracy(predictions, t_train)
			print(f"Iteration {i}: Training Accuracy: {acc:.2f}")

    # Validation
	_, _, _, A2_dev = forward_prop(W1, b1, W2, b2, X_dev)
	dev_preds = np.argmax(A2_dev, axis=0)
	print(f"\nFinal Validation Accuracy: {get_accuracy(dev_preds, t_dev):.2f}")
	test_accuracy = evaluate_model(X_test, t_test, W1, b1, W2, b2)
	print(f"\nFinal Test Accuracy: {evaluate_model(X_test, t_test, W1, b1, W2, b2):.2f}")

    # Visualization of results
	visualize_image(X_train.T, t_train, size)
	visualize_image(X_dev.T, t_dev, size)
	#visualize_split(t_train, t_test)





if __name__ == "__main__":
	main()
